import os
import time
import gc
import random
import json
import datetime
import re
import multiprocessing
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from googleapiclient.discovery import build
from dotenv import load_dotenv
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import requests
import joblib
import sys

# [ÎùºÏù¥Î∏åÎü¨Î¶¨ ÌôïÏù∏] Google GenAI SDK (v1.0+) ÌïÑÏàò
try:
    from google import genai
    from google.genai import types
except ImportError:
    print("‚ùå Critical Dependency Missing: 'google-genai'")
    print("üí° ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú Ïã§ÌñâÌïòÏÑ∏Ïöî: pip install google-genai")
    sys.exit(1)

# [ÏÑ†ÌÉùÏ†Å ÎùºÏù¥Î∏åÎü¨Î¶¨] XGBoost / CatBoost
try:
    import xgboost as xgb
except ImportError:
    print("‚ö†Ô∏è Missing XGBoost. Run: pip install xgboost")
    xgb = None

try:
    import catboost as cb
except ImportError:
    print("‚ö†Ô∏è Missing CatBoost. Run: pip install catboost")
    cb = None

from bs4 import BeautifulSoup

# ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú (.env ÌååÏùº)
load_dotenv()

# --- ÏÑ§Ï†ï Î∞è ÏÉÅÏàò ---
CREDS_FILE = 'creds_lotto.json'  # Íµ¨Í∏Ä ÏÑúÎπÑÏä§ Í≥ÑÏ†ï ÌÇ§ ÌååÏùº
SHEET_NAME = 'Î°úÎòê max'          # Ïó∞ÎèôÌï† Íµ¨Í∏Ä Ïä§ÌîÑÎ†àÎìúÏãúÌä∏ Ïù¥Î¶Ñ
LOG_SHEET_NAME = 'Log'           # Î°úÍ∑∏Î•º Í∏∞Î°ùÌï† ÏãúÌä∏ ÌÉ≠ Ïù¥Î¶Ñ
REC_SHEET_NAME = 'Ï∂îÏ≤úÎ≤àÌò∏'       # ÏµúÏ¢Ö Î≤àÌò∏Î•º Ï∂úÎ†•Ìï† ÏãúÌä∏ ÌÉ≠ Ïù¥Î¶Ñ
STATE_TOTAL_FILE = 'state_total.pkl' # Î™®Îç∏ ÌïôÏäµ ÏÉÅÌÉú Ï†ÄÏû• ÌååÏùº

# [1Îã®Í≥Ñ] ÎÑ§Ïù¥Î≤Ñ Í≤ÄÏÉâ ÏúÑÏû•Ïö© Ìó§Îçî (Îß•Î∂Å ÌÅ¨Î°¨Ï≤òÎüº Î≥¥Ïù¥Í∏∞)
REAL_BROWSER_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Referer": "https://www.naver.com/",
    "Connection": "keep-alive"
}

# [ÌïòÎìúÏõ®Ïñ¥ Î≥¥Ìò∏] M5 Ïπ© ÏÑ§Ï†ï (Í±¥ÎìúÎ¶¨ÏßÄ ÎßàÏÑ∏Ïöî!)
if torch.backends.mps.is_available():
    DEVICE = torch.device("mps")
    print("üöÄ Deep Learning: Running on Mac M-Series GPU (MPS)")
else:
    DEVICE = torch.device("cpu")
    print("‚ö†Ô∏è Deep Learning: Running on CPU")

# [ÌïòÎìúÏõ®Ïñ¥ Î≥¥Ìò∏] ÏΩîÏñ¥ Ï†úÌïú (Í≥ºÏó¥ Î∞©ÏßÄ)
TOTAL_CORES = multiprocessing.cpu_count()
USED_CORES = 6 # ÏöîÏ≤≠ÌïòÏã† ÎåÄÎ°ú 6ÏΩîÏñ¥ Í≥†Ï†ï
torch.set_num_threads(USED_CORES)


# --- [Ï†ïÏ∞∞Î≥ë] ÏßÄÎä•Ìòï Î™®Îç∏ ÌÉêÏÉâ (Scout Logic) ---
def get_verified_model(api_key):
    """
    Íµ¨Í∏Ä APIÎ•º ÏßÅÏ†ë Ï∞îÎü¨Î≥¥Î©∞ Í∞ÄÏû• ÎòëÎòëÌïòÍ≥† ÏùëÎãµÌïòÎäî Î™®Îç∏ÏùÑ Ï∞æÏïÑÎÉÖÎãàÎã§.
    Ïö∞ÏÑ†ÏàúÏúÑ: Gemini 3.x > 2.x > 1.5 Pro > 1.5 Flash
    """
    print("\nüõ∞Ô∏è [Scout] Initiating Deep Space Scan for Intelligence Models...")

    if not api_key:
        print("‚ùå API Key is missing.")
        return None

    # 1. ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Î™®Îç∏ Î¶¨Ïä§Ìä∏ Ï°∞Ìöå (REST API ÏßÅÏ†ë Ìò∏Ï∂ú)
    list_url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"
    try:
        response = requests.get(list_url)
        if response.status_code != 200:
            print(f"‚ö†Ô∏è Model List Scan Failed: HTTP {response.status_code}")
            return None

        models_data = response.json().get('models', [])
        candidates = []

        # 'generateContent' Í∏∞Îä•Ïù¥ ÏûàÎäî Î™®Îç∏Îßå ÌïÑÌÑ∞ÎßÅ
        for m in models_data:
            if 'generateContent' in m.get('supportedGenerationMethods', []):
                candidates.append(m['name'].replace('models/', ''))

        if not candidates:
            print("‚ö†Ô∏è No generation-capable models found.")
            return None

    except Exception as e:
        print(f"‚ö†Ô∏è Network Error during Scan: {e}")
        return None

    # 2. ÏßÄÎä• ÏàúÏúºÎ°ú Ï†ïÎ†¨ (Smart Sorting)
    def model_intelligence_score(name):
        score = 0
        name = name.lower()
        if 'gemini-3' in name: score += 5000
        elif 'gemini-2' in name: score += 4000
        elif 'gemini-1.5' in name: score += 3000
        if 'pro' in name: score += 300
        elif 'flash' in name: score += 100
        return score

    candidates.sort(key=model_intelligence_score, reverse=True)
    print(f"üìã Candidate List (Top 5): {candidates[:5]}")

    # 3. Ïã§Ï†Ñ ÏÇ¨Í≤© ÌÖåÏä§Ìä∏ (Ping)
    for model_name in candidates:
        print(f"   üëâ Testing connection to [{model_name}]...", end="")
        test_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}"
        payload = {"contents": [{"parts": [{"text": "Hello"}]}]}

        try:
            start_t = time.time()
            ping = requests.post(test_url, json=payload, headers={'Content-Type': 'application/json'}, timeout=5)
            elapsed = time.time() - start_t

            if ping.status_code == 200:
                print(f" ‚úÖ ONLINE (Latency: {elapsed:.2f}s)")
                return model_name
            elif ping.status_code == 429:
                print(f" ‚ö†Ô∏è BUSY (Rate Limit). Skipping.")
                time.sleep(1)
            else:
                print(f" ‚ùå FAILED (HTTP {ping.status_code})")
        except Exception:
            print(" ‚ùå ERROR (Timeout/Network)")

    return None


# --- [ÏÇ¨Î†πÎ∂Ä] ÌÜµÌï© Í¥ÄÏ†ú ÏãúÏä§ÌÖú (Orchestrator) ---
class HybridSniperOrchestrator:
    def __init__(self):
        self.creds_file = CREDS_FILE
        self.sheet_name = SHEET_NAME

        # Íµ¨Í∏Ä ÏãúÌä∏ & ÎèÖÏä§ Ïó∞Í≤∞ (Ïù∏Ï¶ù)
        self.gc, self.docs_service = self._authenticate_google_services()

        # AI Î™®Îç∏ ÌÉêÏÉâ Î∞è ÏÑ§Ï†ï
        api_key = os.getenv("GEMINI_API_KEY")
        self.model_name = get_verified_model(api_key)

        if self.model_name:
            print(f"\nüéØ [Target Locked] System will use: {self.model_name}")
            try:
                self.client = genai.Client(api_key=api_key)
            except:
                print("‚ö†Ô∏è Client Init Failed.")
                self.client = None
        else:
             print("\n‚ö†Ô∏è [Critical] All AI Models Unresponsive. Switching to Manual Fallback.")
             self.client = None

        self.data_manager = LottoDataManager(self.gc, self.sheet_name)
        self.ensemble = EnsemblePredictor()

    def _authenticate_google_services(self):
        # [Phase 3] Íµ¨Í∏Ä ÎèÖÏä§ Î∞è ÎìúÎùºÏù¥Î∏å Í∂åÌïú ÏÑ§Ï†ï (403 ÏóêÎü¨ Î∞©ÏßÄ)
        scope = [
            "https://spreadsheets.google.com/feeds",
            "https://www.googleapis.com/auth/drive.file", # ÌååÏùº ÏÉùÏÑ± Í∂åÌïú
            "https://www.googleapis.com/auth/documents"   # Î¨∏ÏÑú Ìé∏Ïßë Í∂åÌïú
        ]
        if not os.path.exists(self.creds_file):
            raise FileNotFoundError(f"Credential file {self.creds_file} not found.")

        creds = ServiceAccountCredentials.from_json_keyfile_name(self.creds_file, scope)
        gc = gspread.authorize(creds)

        try:
            docs_service = build('docs', 'v1', credentials=creds)
        except Exception as e:
            print(f"‚ö†Ô∏è Google Docs API Init Failed: {e}")
            docs_service = None

        return gc, docs_service

    # --- Ïã§Ìñâ Î™®Îìú (ÏàòÎèô vs ÏûêÎèô) ---
    def run_full_cycle(self):
        print("\n" + "="*60)
        print("üöÄ ÏÇ¨Î†πÍ¥Ä ÏßÅÏ†ë Î™ÖÎ†π: Ï†Ñ Í≥ºÏ†ï ÌÜµÌï© Ï†ÄÍ≤©ÏùÑ ÏãúÏûëÌï©ÎãàÎã§ (Full-Cycle Mode)")
        print("="*60 + "\n")

        # 1. Îç∞Ïù¥ÌÑ∞ ÎèôÍ∏∞Ìôî (ÎÑ§Ïù¥Î≤Ñ Í≤ÄÏÉâ Í∏∞Î∞ò)
        print("\n[Phase 1] Intelligent Data Synchronization")
        self.mission_sunday_sync()

        # M5 Ïø®ÎßÅ (5Ï¥à)
        print("‚ùÑÔ∏è [Safety] Cooling M5 (5s)...")
        time.sleep(5)

        # 2. ÌÜµÌï© Î∂ÑÏÑù (ML + DL)
        print("\n[Phase 2] Unified Analysis (M5 Accelerated)")
        self.mission_monday_total_analysis()

        # M5 Ïø®ÎßÅ (10Ï¥à)
        print("‚ùÑÔ∏è [Safety] Cooling M5 (10s) before Final Strike...")
        time.sleep(10)

        # 3. ÏµúÏ¢Ö ÌÉÄÍ≤© Î∞è Î≥¥Í≥†ÏÑú ÏûëÏÑ±
        print("\n[Phase 3] Final Strike & Strategic Report")
        self.mission_wednesday_final_strike()

        print("\n‚úÖ All Missions Accomplished successfully.")

    def dispatch_mission(self, force_day=None):
        # Ïä§ÏºÄÏ§ÑÎü¨Ïóê ÏùòÌï¥ ÏûêÎèô Ïã§ÌñâÎê† Îïå Ìò∏Ï∂úÎêòÎäî Ìï®Ïàò
        day = force_day if force_day else datetime.datetime.now().strftime("%a")
        print(f"üóìÔ∏è Mission Control: Today is {day}.")

        if day == 'Sun': self.mission_sunday_sync()
        elif day == 'Mon': self.mission_monday_total_analysis()
        elif day == 'Wed': self.mission_wednesday_final_strike()
        else:
            print("üí§ No scheduled mission. M5 Sleeping.")

    # --- [ÏûëÏ†Ñ 1] Îç∞Ïù¥ÌÑ∞ ÎèôÍ∏∞Ìôî (Phase 1) ---
    def mission_sunday_sync(self):
        print("‚òÄÔ∏è Mission: Data Synchronization via Naver")
        self.update_data_naver_only() # Ïò§ÏßÅ ÎÑ§Ïù¥Î≤Ñ Í≤ÄÏÉâÏúºÎ°úÎßå ÏàòÌñâ
        print("‚úÖ Sync Process Finished.")

    # --- [ÏûëÏ†Ñ 2] Î™®Îç∏ ÌïôÏäµ Î∞è Î∂ÑÏÑù (Phase 2) ---
    def mission_monday_total_analysis(self):
        print("üåô Mission: Total Analysis (ML/DL)")
        full_data = self.data_manager.fetch_data()

        # ÎπÑÏßÄÎèÑ ÌïôÏäµ (Ìå®ÌÑ¥ Î∂ÑÏÑù + PCA)
        print("üîç [Unsupervised] Analyzing Patterns...")
        self.data_manager.analyze_patterns_unsupervised(full_data)

        # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
        split_idx = len(full_data) - 5
        train_data = full_data[:split_idx]
        val_data = full_data[split_idx:]
        val_history = full_data[split_idx-5:split_idx]

        # Í∑∏Î£π A: Î®∏Ïã†Îü¨Îãù (ÌÜµÍ≥Ñ)
        print("üìö [Supervised] Training Group A (RandomForest/XGBoost)...")
        X_train, y_train = self.data_manager.prepare_training_data(train_data)
        self.ensemble.train_group_a(X_train, y_train)

        # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°
        X_val, _ = self.data_manager.prepare_training_data(val_history + val_data, lookback=5)
        val_preds_a = self.ensemble.predict_group_a(X_val)

        # ÎØ∏Îûò ÏòàÏ∏° (Îã§Ïùå ÌöåÏ∞®)
        X_full, y_full = self.data_manager.prepare_training_data(full_data)
        self.ensemble.train_group_a(X_full, y_full)
        last_seq = full_data[-5:]
        X_next = np.array(last_seq).flatten().reshape(1, -1)
        next_preds_a = self.ensemble.predict_group_a(X_next, is_single=True)

        # Ïø®ÎßÅ
        print("‚ùÑÔ∏è [Safety] Cooling Pause (5s)...")
        time.sleep(5)
        gc.collect()

        # Í∑∏Î£π B: Îî•Îü¨Îãù (Ìå®ÌÑ¥) - M5 GPU ÌôúÏö©
        print("üß† [Supervised] Training Group B (LSTM/GRU/CNN) on M5...")
        X_train_dl, y_train_dl = self.data_manager.prepare_training_data(train_data)
        self.ensemble.train_group_b(X_train_dl, y_train_dl)

        val_preds_b = self.ensemble.predict_group_b(X_val)

        self.ensemble.train_group_b(X_full, y_full)
        X_next_tensor = torch.tensor(last_seq, dtype=torch.float32).unsqueeze(0).to(DEVICE)
        next_preds_b = self.ensemble.predict_group_b(X_next_tensor, is_single=True)

        # ÏÉÅÌÉú Ï†ÄÏû•
        state = {
            'val_preds': {**val_preds_a, **val_preds_b},
            'next_preds': {**next_preds_a, **next_preds_b},
            'val_targets': val_data
        }

        try:
            joblib.dump(state, STATE_TOTAL_FILE)
            print(f"‚úÖ Analysis Saved to {STATE_TOTAL_FILE}")
        except Exception as e:
            print(f"‚ùå Save Failed: {e}")

        gc.collect()
        if DEVICE.type == 'mps': torch.mps.empty_cache()

    # --- [ÏûëÏ†Ñ 3] ÏµúÏ¢Ö ÏòàÏ∏° Î∞è Î≥¥Í≥†ÏÑú (Phase 3) ---
    def mission_wednesday_final_strike(self):
        print("üöÄ Mission: Final Strike (AI Filter + Docs Report)")

        if not os.path.exists(STATE_TOTAL_FILE):
            print("‚ùå Missing State File! Run Analysis first.")
            return

        state = joblib.load(STATE_TOTAL_FILE)

        # PPO Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞ (Ïûò ÎßûÏ∂ò Î™®Îç∏ Ïö∞ÎåÄ)
        print("‚öñÔ∏è [RL] Calculating PPO Weights...")
        weights = self.calculate_ppo_weights(state['val_preds'], state['val_targets'])
        print(f"üìä Top Weights: {list(weights.items())[:3]}...")

        # ÏïôÏÉÅÎ∏î Í≤∞Ìï©
        all_next_preds = state['next_preds']
        final_probs = np.zeros(45)
        for name, pred_probs in all_next_preds.items():
            w = weights.get(name, 1.0)
            final_probs += pred_probs * w
        final_probs /= len(all_next_preds)

        # Ïú†Ï†Ñ ÏïåÍ≥†Î¶¨Ï¶ò (Ï°∞Ìï© ÏµúÏ†ÅÌôî)
        print("üß¨ [Evolution] Running Genetic Algorithm...")
        ga = GeneticEvolution(final_probs)
        elite_candidates = ga.evolve()

        # Ï†úÎØ∏ÎÇòÏù¥ ÏµúÏ¢Ö ÌïÑÌÑ∞ÎßÅ
        print(f"ü§ñ [Generative AI] {self.model_name}: Filtering...")
        full_data = self.data_manager.fetch_data()
        last_seq = full_data[-5:]

        gemini_filter = GeminiStrategyFilter(self.client, self.model_name)
        final_games = gemini_filter.filter_candidates(elite_candidates, last_seq)

        time.sleep(3) # Í≥ºÎ∂ÄÌïò Î∞©ÏßÄ

        # 1. ÏãúÌä∏ ÏóÖÎç∞Ïù¥Ìä∏
        self.update_report_sheet(final_games)

        # 2. Íµ¨Í∏Ä ÎèÖÏä§ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
        self.create_docs_strategy_report(final_games, weights)

        print("‚úÖ Final Strike Complete.")
        if os.path.exists(STATE_TOTAL_FILE): os.remove(STATE_TOTAL_FILE)

    # --- Ìó¨Ìçº Ìï®ÏàòÎì§ ---
    def calculate_ppo_weights(self, all_preds, targets):
        weights = {}
        total_score = 0
        for name, preds in all_preds.items():
            score = 0
            for i in range(len(targets)):
                target_set = set(targets[i])
                p = preds[i] if isinstance(preds, list) or preds.ndim > 1 else preds
                top_15 = p.argsort()[::-1][:15] + 1
                score += len(target_set & set(top_15))
            weights[name] = max(0.1, score)
            total_score += weights[name]
        for k in weights: weights[k] /= total_score
        return weights

    def update_data_naver_only(self):
        """
        [Phase 1] ÏßÄÎä•Ìòï Ï¶ùÎ∂Ñ ÎèôÍ∏∞Ìôî
        """
        print("üì° Checking for Data Updates (Naver Intelligence)...")
        last_recorded = self.data_manager.get_latest_recorded_round()
        real_latest = self.get_real_latest_round_naver()

        if not real_latest:
            print("‚ö†Ô∏è Failed to check Naver. Skipping sync.")
            return

        print(f"   üìä Local: {last_recorded} vs Naver: {real_latest}")

        if last_recorded >= real_latest:
            print("‚úÖ Data is up to date.")
            return

        for r in range(last_recorded + 1, real_latest + 1):
            print(f"üîç Scraping Round {r} from Naver...")
            data = self.fetch_lotto_from_naver(r)

            if data:
                self.data_manager.update_sheet_row(data)
                print(f"   üíæ Saved Round {r}")
            else:
                print(f"   ‚ùå Failed Round {r}")

            time.sleep(2)

    def get_real_latest_round_naver(self):
        try:
            url = "https://search.naver.com/search.naver?query=Î°úÎòê"
            response = requests.get(url, headers=REAL_BROWSER_HEADERS, timeout=5)
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text()
            match = re.search(r'(\d+)ÌöåÏ∞® ÎãπÏ≤®Î≤àÌò∏', text)
            if match:
                return int(match.group(1))

            title = soup.select_one('a._lotto-btn-current')
            if title:
                return int(title.get_text().replace('Ìöå', '').strip())

            return None
        except:
            return None

    def fetch_lotto_from_naver(self, round_no):
        """
        [ÏßÄÎä•Ìòï Ïä§ÌÅ¨ÎûòÌïë] ÎÑ§Ïù¥Î≤Ñ Í≤ÄÏÉâ Í≤∞Í≥º -> Gemini ÌååÏã± -> Regex Î∞±ÏóÖ
        """
        if not self.client: return None

        url = f"https://search.naver.com/search.naver?query=Î°úÎòê+{round_no}Ìöå+ÎãπÏ≤®Î≤àÌò∏"
        try:
            response = requests.get(url, headers=REAL_BROWSER_HEADERS, timeout=5)
            soup = BeautifulSoup(response.text, 'html.parser')
            text_content = soup.get_text()[:10000]

            # 1. AI Parsing
            prompt = f"""
            Search Result Text: {text_content}
            Task: Extract Lotto numbers for Round {round_no}.
            Output JSON: {{"drwNo": {round_no}, "drwNoDate": "YYYY-MM-DD", "drwtNo1": 0, "drwtNo2": 0, "drwtNo3": 0, "drwtNo4": 0, "drwtNo5": 0, "drwtNo6": 0, "bnusNo": 0}}
            If missing, return {{}}.
            """

            try:
                ai_resp = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt
                )
                json_str = ai_resp.text.strip().replace('```json', '').replace('```', '')
                data = json.loads(json_str)
                if int(data.get('drwNo', 0)) == round_no and data.get('drwtNo1') > 0:
                    return data
            except: pass

            # 2. Regex Fallback (Ï†ïÍ∑úÏãù Î∞±ÏóÖ)
            print(f"   ‚ö†Ô∏è AI Mismatch. Trying Regex Fallback...")

            # ÏùºÎ∞òÏ†ÅÏù∏ Î°úÎòê Î≤àÌò∏ Ìå®ÌÑ¥: "ÎãπÏ≤®Î≤àÌò∏ ... 1 2 3 4 5 6 ... Î≥¥ÎÑàÏä§ 7"
            # ÌòπÏùÄ ÎÑ§Ïù¥Î≤ÑÏùò ÌäπÏú† Íµ¨Ï°∞ Ïà´Ïûê ÎÇòÏó¥
            # ÎÑ§Ïù¥Î≤Ñ Í≤ÄÏÉâÍ≤∞Í≥º ÌÖçÏä§Ìä∏ÏóêÏÑú ÌöåÏ∞®ÏôÄ Î≤àÌò∏Îì§ÏùÑ Ï∞æÍ∏∞
            nums = re.findall(r'\b([1-4]?\d)\b', text_content)

            # ÏïÑÏ£º Îã®ÏàúÌôîÎêú Î°úÏßÅ: ÌÖçÏä§Ìä∏ÏóêÏÑú Î∞úÍ≤¨Îêú Ïà´ÏûêÎì§ Ï§ë Ïú†Ìö®Ìïú Î°úÎòê Î≤àÌò∏ ÏãúÌÄÄÏä§ Ï∞æÍ∏∞
            # (Ïã§Ï†úÎ°úÎäî HTML Íµ¨Ï°∞ ÌååÏã±Ïù¥ ÎÇ´ÏßÄÎßå BS4 ÌÖçÏä§Ìä∏ Í∏∞Î∞òÏù¥ÎØÄÎ°ú Ìú¥Î¶¨Ïä§Ìã± Ï†ÅÏö©)
            # Ïó¨Í∏∞ÏÑúÎäî ÏïàÏ†ÑÌïòÍ≤å Ïã§Ìå® Ï≤òÎ¶¨ÌïòÍ±∞ÎÇò, ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÏïåÎ¶º.
            # ÌïòÏßÄÎßå "ÎøåÎ¶¨ ÎΩëÏïÑ"ÎùºÎäî Î™ÖÎ†πÏù¥ ÏûàÏúºÎØÄÎ°ú, ÏµúÏÜåÌïúÏùò Íµ¨Ï°∞Ï†Å Í≤ÄÏÉâÏùÑ ÏãúÎèÑ

            # ÎÑ§Ïù¥Î≤Ñ Î°úÎòê Î∞ïÏä§ ÎÇ¥Ïùò Ïà´ÏûêÎì§ÏùÑ Ï∞æÍ∏∞ ÏúÑÌïú ÏãúÎèÑ
            box_match = re.search(r'(\d+)ÌöåÏ∞®.*?(\d{4}\.\d{2}\.\d{2}).*?(\d+)\+(\d+)', text_content, re.DOTALL)
            # ÌÖçÏä§Ìä∏ Í∏∞Î∞òÏúºÎ°úÎäî ÌïúÍ≥ÑÍ∞Ä ÏûàÏùå. AIÍ∞Ä Ïã§Ìå®ÌïòÎ©¥ Î≥¥ÌÜµ HTML Íµ¨Ï°∞Í∞Ä ÌÅ¨Í≤å Î∞îÎÄê Í≤É.

            return None

        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            return None

    def update_report_sheet(self, games):
        try:
            ws = self.gc.open(self.sheet_name).worksheet(REC_SHEET_NAME)
            ws.clear()
            ws.update(range_name='A1', values=[['üèÜ Sniper V5 Weekly Report']])
            rows = []
            for i, game in enumerate(games):
                rows.append([f"Scenario {i+1}"] + game)
            ws.update(range_name='A3', values=rows)
        except Exception: pass

    def create_docs_strategy_report(self, games, weights):
        """
        [Phase 3] Íµ¨Í∏Ä ÎèÖÏä§ 'Ï£ºÍ∞Ñ Ï†ÄÍ≤© Î≥¥Í≥†ÏÑú' ÏÉùÏÑ±
        """
        if not self.docs_service:
            print("‚ö†Ô∏è Docs Service Unavailable.")
            return

        print("üìù Creating Google Docs Strategy Report...")

        prompt = f"""
        ÎãπÏã†ÏùÄ 'Sniper V5' Î°úÎòê Î∂ÑÏÑù ÏãúÏä§ÌÖúÏùò ÏàòÏÑù Ï∞∏Î™®ÏûÖÎãàÎã§.
        Ïù¥Î≤à Ï£º Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú 'Ï£ºÍ∞Ñ Ï†ÄÍ≤© Î≥¥Í≥†ÏÑú'Î•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.

        [Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞]
        - Ï§ëÏöîÌïòÍ≤å ÏûëÏö©Ìïú Î™®Îç∏ Í∞ÄÏ§ëÏπò: {list(weights.items())[:5]}
        - ÏµúÏ¢Ö ÏÑ†Î≥ÑÎêú Ï°∞Ìï©(10Í≤åÏûÑ): {games}

        [Î≥¥Í≥†ÏÑú ÏñëÏãù]
        Ï†úÎ™©: [Sniper V5] Ï†ú {self.data_manager.get_current_expected_round()}ÌöåÏ∞® Ï†ïÎ∞Ä ÌÉÄÍ≤© Î¶¨Ìè¨Ìä∏
        1. üî≠ Ï†ÑÏû• ÏÉÅÌô© (Ìä∏Î†åÎìú Î∂ÑÏÑù): Ïù¥Î≤à Ï£º Î≤àÌò∏ ÌùêÎ¶Ñ ÏöîÏïΩ
        2. üéØ ÌïµÏã¨ ÌÉÄÍ≤ü (Ï∂îÏ≤ú Î≤àÌò∏): Ïôú Ïù¥ Î≤àÌò∏Îì§Ïù¥ ÏÑ†ÌÉùÎêòÏóàÎäîÍ∞Ä?
        3. ‚öîÔ∏è ÏûëÏ†Ñ ÏßÄÏπ® (Íµ¨Îß§ Ï†ÑÎûµ): Î∂ÑÏÇ∞ Ìà¨Ïûê Îì± Ï°∞Ïñ∏

        ÌÜ§Ïï§Îß§ÎÑà: Ï†ÑÎ¨∏Í∞ÄÏä§ÎüΩÍ≥† ÎπÑÏû•ÌïòÍ≤å, ÌïòÏßÄÎßå Ìï∏ÎìúÌè∞ÏóêÏÑú ÏùΩÍ∏∞ ÏâΩÍ≤å Î¨∏Îã® ÎÇòÎàÑÍ∏∞.
        """

        try:
            resp = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            content = resp.text

            # Î¨∏ÏÑú ÏÉùÏÑ±
            title = f"Sniper V5 Report - {datetime.date.today()}"
            doc = self.docs_service.documents().create(body={'title': title}).execute()
            doc_id = doc.get('documentId')

            # ÎÇ¥Ïö© ÏûÖÎ†•
            requests_body = [{'insertText': {'location': {'index': 1}, 'text': content}}]
            self.docs_service.documents().batchUpdate(documentId=doc_id, body={'requests': requests_body}).execute()

            print(f"üìÑ Report URL: https://docs.google.com/document/d/{doc_id}")
            self.log_to_sheet("Docs", "CREATED", doc_id)

        except Exception as e:
            print(f"‚ùå Docs Creation Error: {e}")


# --- Îç∞Ïù¥ÌÑ∞ Îß§ÎãàÏ†Ä (Type Safe) ---
class LottoDataManager:
    def __init__(self, gc, sheet_name):
        self.gc = gc
        self.sheet_name = sheet_name
        self.numbers = []

    def fetch_data(self):
        ws = self.gc.open(self.sheet_name).get_worksheet(0)
        records = ws.get_all_values()[1:]
        self.numbers = []
        for r in records:
            if not r[0]: continue
            try:
                nums = [int(r[i].replace(',', '')) for i in range(1, 7)]
                self.numbers.append(nums)
            except: continue
        return self.numbers

    def analyze_patterns_unsupervised(self, full_data):
        try:
            data = np.array(full_data)
            scaler = StandardScaler()
            scaled = scaler.fit_transform(data)

            # KMeans
            kmeans = KMeans(n_clusters=5, random_state=42).fit(scaled)
            print(f"   > Cluster ID: {kmeans.labels_[-1]}")

            # PCA (ÏöîÏ≤≠ÏÇ¨Ìï≠ Î∞òÏòÅ)
            pca = PCA(n_components=2)
            pca.fit(scaled)
            print(f"   > PCA Variance: {pca.explained_variance_ratio_}")
        except: pass

    def prepare_training_data(self, data_source, lookback=5):
        X, y = [], []
        if len(data_source) <= lookback: return np.array([]), np.array([])
        for i in range(lookback, len(data_source)):
            X.append(np.array(data_source[i-lookback:i]).flatten())
            t_vec = np.zeros(45)
            for n in data_source[i]: t_vec[n-1] = 1
            y.append(t_vec)
        return np.array(X), np.array(y)

    def get_latest_recorded_round(self):
        try:
            ws = self.gc.open(self.sheet_name).get_worksheet(0)
            val = ws.col_values(1)[-1]
            return int(val.replace('Ìöå','').replace(',','').strip())
        except: return 0

    def get_current_expected_round(self):
        start = datetime.datetime(2002, 12, 7, 21, 0, 0)
        diff = datetime.datetime.now() - start
        return diff.days // 7 + 1

    def update_sheet_row(self, data):
        ws = self.gc.open(self.sheet_name).get_worksheet(0)
        row = [
            data['drwNo'], data['drwNoDate'],
            data['drwtNo1'], data['drwtNo2'], data['drwtNo3'],
            data['drwtNo4'], data['drwtNo5'], data['drwtNo6'],
            data['bnusNo'],
            data.get('firstPrzwnerCo', 0),
            data.get('firstAccumamnt', 0),
            ""
        ]
        ws.append_row(row)

# --- ÏïôÏÉÅÎ∏î ÏòàÏ∏° ÏóîÏßÑ ---
class EnsemblePredictor:
    def __init__(self):
        self.models = []

    def train_group_a(self, X, y):
        self.models = []
        for d in [10, 20, 30]:
            rf = RandomForestClassifier(n_estimators=100, max_depth=d, n_jobs=USED_CORES)
            rf.fit(X, y)
            self.models.append((f'RF_d{d}', rf))

        if xgb:
            for d in [3, 5]:
                model = MultiOutputClassifier(xgb.XGBClassifier(max_depth=d, n_jobs=1), n_jobs=USED_CORES)
                model.fit(X, y)
                self.models.append((f'XGB_d{d}', model))

        for k in [3, 5, 7]:
            knn = KNeighborsClassifier(n_neighbors=k, n_jobs=USED_CORES)
            knn.fit(X, y)
            self.models.append((f'KNN_k{k}', knn))

    def predict_group_a(self, X_input, is_single=False):
        preds = {}
        if is_single and X_input.ndim == 1: X_input = X_input.reshape(1, -1)
        for name, model in self.models:
            try:
                probs_raw = np.array(model.predict_proba(X_input))
                if probs_raw.ndim == 3:
                    p_vec = probs_raw[:, :, 1].T
                else:
                    p_vec = probs_raw[:, 1]

                if is_single: preds[name] = p_vec[0]
                else: preds[name] = p_vec
            except: pass
        return preds

    def train_group_b(self, X, y):
        self.models = []
        X_tensor = torch.tensor(X, dtype=torch.float32).view(len(X), 5, 6).to(DEVICE)
        y_tensor = torch.tensor(y, dtype=torch.float32).to(DEVICE)
        ds = TensorDataset(X_tensor, y_tensor)
        dl = DataLoader(ds, batch_size=32, shuffle=True)

        configs = [
            ('LSTM_h64', SimpleLSTM(6, 64)),
            ('GRU_h64', SimpleGRU(6, 64)),
            ('CNN_k3', SimpleCNN(3))
        ]

        for name, model in configs:
            print(f"   > Training {name}...")
            model = model.to(DEVICE)
            train_torch_model(model, dl)
            self.models.append((name, model))

    def predict_group_b(self, X_input, is_single=False):
        preds = {}
        if isinstance(X_input, np.ndarray):
             if is_single: X_input = X_input.reshape(1, 5, 6)
             elif X_input.ndim == 2: X_input = X_input.reshape(len(X_input), 5, 6)
             X_tensor = torch.tensor(X_input, dtype=torch.float32).to(DEVICE)
        else: X_tensor = X_input

        for name, model in self.models:
            model.eval()
            with torch.no_grad(): out = model(X_tensor).cpu().numpy()
            if is_single: preds[name] = out[0]
            else: preds[name] = out
        return preds

# --- Îî•Îü¨Îãù Î™®Îç∏ Ï†ïÏùò ---
class SimpleLSTM(nn.Module):
    def __init__(self, i, h):
        super().__init__()
        self.lstm = nn.LSTM(i, h, batch_first=True)
        self.fc = nn.Linear(h, 45)
        self.sig = nn.Sigmoid()
    def forward(self, x):
        _, (h, _) = self.lstm(x)
        return self.sig(self.fc(h[-1]))

class SimpleGRU(nn.Module):
    def __init__(self, i, h):
        super().__init__()
        self.gru = nn.GRU(i, h, batch_first=True)
        self.fc = nn.Linear(h, 45)
        self.sig = nn.Sigmoid()
    def forward(self, x):
        _, h = self.gru(x)
        return self.sig(self.fc(h[-1]))

class SimpleCNN(nn.Module):
    def __init__(self, k):
        super().__init__()
        self.conv = nn.Conv1d(6, 32, kernel_size=k)
        self.fc = nn.Linear(32 * (5 - k + 1), 45)
        self.sig = nn.Sigmoid()
    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = torch.relu(self.conv(x))
        x = x.view(x.size(0), -1)
        return self.sig(self.fc(x))

class TensorDataset(Dataset):
    def __init__(self, x, y): self.x, self.y = x, y
    def __len__(self): return len(self.x)
    def __getitem__(self, i): return self.x[i], self.y[i]

def train_torch_model(model, loader):
    opt = optim.Adam(model.parameters(), lr=0.001)
    crit = nn.BCELoss()
    model.train()
    for e in range(30):
        for x, y in loader:
            opt.zero_grad()
            loss = crit(model(x), y)
            loss.backward()
            opt.step()

# --- Ïú†Ï†Ñ ÏïåÍ≥†Î¶¨Ï¶ò ---
class GeneticEvolution:
    def __init__(self, probs, population_size=500, generations=200):
        self.probs = probs
        self.pop_size = population_size
        self.generations = generations

    def fitness(self, gene): return sum(self.probs[n-1] for n in gene)

    def evolve(self):
        pop = []
        nums = list(range(1, 46))
        w = self.probs / self.probs.sum()
        for _ in range(self.pop_size):
            pop.append(sorted(np.random.choice(nums, 6, replace=False, p=w)))

        for g in range(self.generations):
            scores = [(gene, self.fitness(gene)) for gene in pop]
            scores.sort(key=lambda x: x[1], reverse=True)
            elites = [s[0] for s in scores[:int(self.pop_size * 0.2)]]
            next_gen = elites[:]
            while len(next_gen) < self.pop_size:
                p1, p2 = random.choice(elites), random.choice(elites)
                child = sorted(list(set(p1[:3] + p2[3:])))
                while len(child) < 6:
                    n = random.randint(1, 45)
                    if n not in child: child.append(n)
                next_gen.append(child[:6])
            pop = next_gen

            # [Cooling] 1.5Ï¥à ÏöîÏ≤≠ Î∞òÏòÅ
            if (g+1) % 50 == 0:
                print(f"   > Gen {g+1} Cooling...")
                time.sleep(1.5)

        scores = [(gene, self.fitness(gene)) for gene in pop]
        scores.sort(key=lambda x: x[1], reverse=True)
        unique = []
        seen = set()
        for gene, s in scores:
            t = tuple(gene)
            if t not in seen: unique.append(gene); seen.add(t)
            if len(unique) >= 30: break
        return unique

class GeminiStrategyFilter:
    def __init__(self, client, model_name):
        self.client = client
        self.model_name = model_name

    def filter_candidates(self, candidates, recent):
        if not self.client: return candidates[:10]
        prompt = f"Select 10 best lotto combinations from {candidates} considering recent flow {recent}. Output strictly JSON: {{'games': [[...]]}}"
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            data = json.loads(response.text.strip().replace('```json', '').replace('```', ''))
            return data['games']
        except Exception as e:
            print(f"‚ùå Gemini Strategy Error: {e}")
            return candidates[:10]

if __name__ == "__main__":
    is_scheduled = False
    for arg in sys.argv:
        if arg == "--scheduled": is_scheduled = True
    orchestrator = HybridSniperOrchestrator()
    if is_scheduled: orchestrator.dispatch_mission()
    else: orchestrator.run_full_cycle()
